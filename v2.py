# -*- coding: utf-8 -*-
"""V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1flYlnIDLueQ4vUMmxxz87MeRCA8lHVWn

Predicting California Real Estate Price

Context: Buying a home is one of the most important decisions you make over the course
of your lifetime. The decision significantly impacts quality of life and can be 
a profitable investment. Predicting what the cost of a home is based on certain details
and elements of the house would help to make a smarter decision when looking to purchase.
With this ability, the average family would be able to get the most out of their investment 
given their budget. In this report, we strive to empower the buyer and provide further 
insight into predicting how much a home costs in California.

Our dataset comes from Kaggle and can be found at: 

https://www.kaggle.com/yellowj4acket/real-estate-california
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import math
from scipy import stats
import matplotlib.pyplot as plt
"""import sys
from pathlib import Path
from zipfile import ZipFile"""

"""# download data set
# https://www.kaggle.com/unsdsn/world-happiness?select=2017.csv
dataset = "unsdsn/world-happiness"
sys.argv = [sys.argv[0]] + f"datasets download {dataset}".split(" ")
kaggle.cli.main()

zfile = ZipFile(f"{dataset.split('/')[1]}.zip")

dfs = {f.filename:pd.read_csv(zfile.open(f)) for f in zfile.infolist() }

dfs["2017.csv"]"""

path = "/content/drive/MyDrive/python_for_data_scientists/Capstone 2/RealEstate_California.csv"

df = pd.read_csv(path)

df['is_bankOwned'] = df['is_bankOwned'].map({1: 'yes', 0: 'no'})
df['is_forAuction'] = df['is_forAuction'].map({1: 'yes', 0: 'no'})
df['hasBadGeocode'] = df['hasBadGeocode'].map({1: 'yes', 0: 'no'})
df['hasGarage'] = df['hasGarage'].map({1: 'yes', 0: 'no'})
df['pool'] = df['pool'].map({1: 'yes', 0: 'no'})
df['spa'] = df['spa'].map({1: 'yes', 0: 'no'})
df['isNewConstruction'] = df['isNewConstruction'].map({1: 'yes', 0: 'no'})
df['hasPetsAllowed'] = df['hasPetsAllowed'].map({1: 'yes', 0: 'no'})

"""The research question we want to answer is:

On average, do houses that are built before 1980 with four bedrooms cost more than 
houses that were built in or after 1980 with 3 bedrooms?

We are curious as to what costs more in the housing market from our sample.
Does an older and slightly bigger house cost more than a newer and slightly smaller house?"""

"""To observe the average of our two sample populations, we will use the following code to
make variables "before" and "after"""

before = df[df["yearBuilt"] < 1980]
before = df[df["bedrooms"] == 4]


after = df[df["yearBuilt"] >= 1980]
after = df[df["bedrooms"] == 3]

before.shape

"""There are 7,228 houses in the before sample population"""

after.shape

"""There are 10,911 houses in the after sample population"""

"""Now let's find the average price of the two sample populations to compare"""

before["price"].mean()

"""The average price for the "before" sample population is $1,284,797"""

after["price"].mean()

"""The average price for the "after" sample population is $888,022"""

# Plot our variables, we need kurtosis and skewness to be between -3 and 3 for a
#t-test to work properly


plt.hist(before['price'], alpha = .5, label = "before")
plt.hist(after['price'], alpha = .5, label = "after")
plt.legend(loc="upper right")
plt.show()

print(stats.describe(before['price']))
print(stats.describe(after['price']))

#Skewness and kurtosis are too high to move forward

#Is the sale price on average higher for homes with a pool?

pool_yes = df[df['pool'] == 'yes']
pool_yes.info()

pool_no = df[df['pool'] == 'no']
pool_no.info()

plt.hist(pool_yes['price'], alpha = .5, label = "yes")
plt.hist(pool_no['price'], alpha = .5, label = "no")
plt.legend(loc="upper right")
plt.show()

print(stats.describe(pool_yes['price']))
print(stats.describe(pool_no['price']))

#Skewness and kurtosis are too high again

#Is the sale price on average higher for homes with a spa?

spa_yes = df[df['spa'] == 'yes']
spa_yes.info()

spa_no = df[df['spa'] == 'no']
spa_no.info()

plt.hist(spa_yes['price'], alpha = .5, label = "yes")
plt.hist(spa_no['price'], alpha = .5, label = "no")
plt.legend(loc="upper right")
plt.show()

print(stats.describe(spa_yes['price']))
print(stats.describe(spa_no['price']))

#skewness and kurtosis are too high again

#Is the sale price on average higher for homes with a garage?

hasGarage_yes = df[df['hasGarage'] == 'yes']
hasGarage_yes.info()

hasGarage_no = df[df['hasGarage'] == 'no']
hasGarage_no.info()

plt.hist(hasGarage_yes['price'], alpha = .5, label = "yes")
plt.hist(hasGarage_no['price'], alpha = .5, label = "no")
plt.legend(loc="upper right")
plt.show()

print(stats.describe(hasGarage_yes['price']))
print(stats.describe(hasGarage_no['price']))

#Even though all of the assumptions for the t-test are not met, we will run the t-test

stats.ttest_ind(hasGarage_yes['price'], hasGarage_no['price'])

"""The t-statistic is greater than 1.96 which satisfies rejecting the null
hypothesis, however the p-value is nowhere near being at or below 0.05 meaning the test isn't significant. 
There is a high probability that you would find an effect at least as extreme as this sample's, 
assuming that the null is true."""

# Lets calculate the 95% confidence interval

def get_95_ci(array_1, array_2):
    sample_1_n = array_1.shape[0]
    sample_2_n = array_2.shape[0]
    sample_1_mean = array_1.mean()
    sample_2_mean = array_2.mean()
    sample_1_var = array_1.var()
    sample_2_var = array_2.var()
    mean_difference = sample_2_mean - sample_1_mean
    std_err_difference = math.sqrt((sample_1_var/sample_1_n)+(sample_2_var/sample_2_n))
    margin_of_error = 1.96 * std_err_difference
    ci_lower = mean_difference - margin_of_error
    ci_upper = mean_difference + margin_of_error
    return("The difference in means at the 95% confidence interval (two-tail) is between "+str(ci_lower)+" and "+str(ci_upper)+".")

get_95_ci(hasGarage_yes['price'], hasGarage_no['price'])

# Create a point plot to compare the means

import seaborn as sns

g = sns.pointplot(data=[hasGarage_yes['price'], hasGarage_no['price']], join=False)

g.set(xticklabels = ['hasGarage_yes', 'hasGarage_no'])

#Let's take a preliminary look at our data

df.info()

df.columns

df.head()

#It looks like there is a good mix of numerical and 
#catergorical variables in this dataset. There are 39 variables and 
#35389 entries in total

#Now that we've seen the data, let's explore it with some EDA

df.nunique()

#df.unique() gives us all of the unique values for each column.

#It looks like there are 10 variables that have binary
#data which is represented by a 2 

#id, countyId, streetAddress, longitude, latitude and 
#description all have primarily unique values for each entry.

#stateId, state, country and currency all only have one 
#unique value which makes sense because the dataset is of 
#california housing, using the same currency and located in 
#the same country

#Let's do an initial search for missing values in our dataset
df.isnull()

#As we can see above, there are plenty of missing values. However, the above 
#is only a snippet of the bigger picture

#To get a better idea of how many missing values there are 
#in our dataset let's get some percentages of missing values for 
#each variable

#Percentage of missing values per column

df.isnull().sum()*100/df.isnull().count()

#There appears to be a small amount of missing values in the datePostedString,
#time, zipcode, and description columns. <1% each.

#Since we will not lose a significant amount of information from these records
#by eliminating them, we will decide to do so. This way we can
#continue to analyze our data as a whole without any gaps that might deter
#our calculations.

#When trying to plot the log of the continuous variables, I found that
#there are zero values in the price column.
#We will drop these as well as they functionally act the same as
#null or missing values

#Count of 0 values in price column
print((df.price.values == 0).sum())


#Count of 0 values in pricePerSquareFoot column
print((df.pricePerSquareFoot.values == 0).sum())

print((df.yearBuilt.values == 0).sum())
print((df.livingArea.values == 0).sum())
print((df.livingAreaValue.values == 0).sum())
print((df.bathrooms.values == 0).sum())
print((df.bedrooms.values == 0).sum())
print((df.buildingArea.values == 0).sum())
print((df.garageSpaces.values == 0).sum())

"""Above, we calculated all of the zero values. Zero value might hinder out calculations later
in our analysis. Let's try to understand what the zero value mean and how to handle them.

 - 'price' column zero values represent a mistake in recording of the data
 - same for 'pricePerSquareFoot'
 - 'yearBuilt' column zero values could represent a mistake in recording of the data 
 if there is a house on the real estate because it can't be built in year '0'. However, if there
 is not house built on the land, then this zero value may make sense.
 - 

"""

df.loc[df['price'] == 0, 'price'] = np.nan
df.loc[df['pricePerSquareFoot'] == 0, 'price'] = np.nan
df.loc[df['yearBuilt'] == 0, 'price'] = np.nan

#Now let's replace all fo the zero values with null values and drop them so they don't
#stop our natural log from running in the future.

"""cols = ["price","pricePerSquareFoot","id","countyId","cityId","time","yearBuilt",
        "zipcode","longitude","latitude","livingArea","livingAreaValue","bathrooms",
        "bedrooms","buildingArea","garageSpaces"]

df[cols] = df[cols].replace({0:np.nan})"""

df.dropna(inplace = True)

#Count of 0 values in price column
print((df.price.values == 0).sum())

#Count of 0 values in pricePerSquareFoot column
print((df.pricePerSquareFoot.values == 0).sum())

print((df.yearBuilt.values == 0).sum())

#Now let's test for outliers. Outliers may hinder the reliability of our predictions just as
#null values could. We want to handle them in order to increase our accuracy of anaysis.

#We will start with statistical tests

from scipy.stats import zscore

z_scores = zscore(df["price"])
for threshold in range(1,5):
   print("The score threshold is: {}".format(threshold))
   print("The indices of the outliers:")
   print(np.where(z_scores > threshold))
   print("Number of outliers is: {}".format(len((np.where(z_scores > threshold)[0]))))

#From the test above it seems that we have no outliers, which is odd, but good.

#Now let's visually inspect our data for outliers to confirm the statistical test.

import matplotlib.pyplot as plt

plt.boxplot(df["price"])
plt.title("Box plot of price (whis=1.5)")
plt.show()

plt.boxplot(df["price"], whis=20)
plt.title("Box plot of price (whis=20)")
plt.show()

#The boxplot shows there are only outliers on the high end

#Let's confirm with a histogram

plt.hist(df["price"])
plt.title("Histogram of price")
plt.show()

#The histogram confirms that the outliers are only on the high end.

#We will use one way winsorization, that should solve the outliers

from scipy.stats.mstats import winsorize

df["win_price"] = winsorize(df["price"], (0, 0.10))

plt.boxplot(df["win_price"], whis=20)
plt.title("Box plot of win_price (whis=20)")
plt.show()

plt.boxplot(df["win_price"])
plt.title("Box plot of win_price (whis=1.5)")
plt.show()

df["win_pricePerSquareFoot"] = winsorize(df["pricePerSquareFoot"], (0, 0.10))
"""df["win_id"] = winsorize(df["id"], (0, 0.10))
df["win_countyId"] = winsorize(df["countyId"], (0, 0.10))
df["win_cityId"] = winsorize(df["cityId"], (0, 0.10))"""
df["win_time"] = winsorize(df["time"], (0, 0.10))
df["win_yearBuilt"] = winsorize(df["yearBuilt"], (0, 0.10))
"""df["win_zipcode"] = winsorize(df["zipcode"], (0, 0.10))
df["win_longitude"] = winsorize(df["longitude"], (0, 0.10))
df["win_latitude"] = winsorize(df["latitude"], (0, 0.10))"""
df["win_livingArea"] = winsorize(df["livingArea"], (0, 0.10))
df["win_livingAreaValue"] = winsorize(df["livingAreaValue"], (0, 0.10))
df["win_bathrooms"] = winsorize(df["bathrooms"], (0, 0.10))
df["win_bedrooms"] = winsorize(df["bedrooms"], (0, 0.10))
df["win_buildingArea"] = winsorize(df["buildingArea"], (0, 0.10))
df["win_garageSpaces"] = winsorize(df["garageSpaces"], (0, 0.10))

#We have transformed the outliers successfully.

#Now for some EDA. We can leave out ID, countyID, cityID, zipcode, Longitude and Latitude.
#I will leave them out of my EDA and only use them to engineer
#new features or earn something about other features.

#For organization sake, let's make a list of our continuous
#and categorical variables that we plan to explore with EDA

continuous = ["price","pricePerSquareFoot","time","yearBuilt","livingArea","livingAreaValue","bathrooms",
        "bedrooms","buildingArea","garageSpaces", "win_price", 
        "win_pricePerSquareFoot", "win_time",
        "win_yearBuilt", "win_livingArea", "win_livingAreaValue",
        "win_bathrooms", "win_bedrooms", "win_buildingArea", "win_garageSpaces"]

categorical = ["is_bankOwned", "is_forAuction", "event", "hasBadGeocode",
               "lotAreaUnits", "hasGarage", "pool", "spa",
               "isNewConstruction", "hasPetsAllowed", "homeType"]

import seaborn as sns
import scipy.stats as stats

df.describe()

df.describe(include=['O'])

df.describe(include= 'all')

#Univariate visualization of continuous variables

plt.figure(figsize=(20,20))

# Histograms of the original data
plt.subplot(4, 3, 1)
plt.hist(df["price"])
plt.title("histogram of price")

plt.subplot(4, 3, 2)
plt.hist(df["pricePerSquareFoot"])
plt.title("histogram of pricePerSquareFoot")

plt.subplot(4, 3, 3)
plt.hist(df["time"])
plt.title("histogram of time")

plt.subplot(4, 3, 4)
plt.hist(df["yearBuilt"])
plt.title("histogram of yearBuilt")

plt.subplot(4, 3, 5)
plt.hist(df["livingArea"])
plt.title("histogram of livingArea")

plt.subplot(4, 3, 6)
plt.hist(df["livingAreaValue"])
plt.title("histogram of livingAreaValue")

plt.subplot(4, 3, 7)
plt.hist(df["bathrooms"])
plt.title("histogram of bathrooms")

plt.subplot(4, 3, 8)
plt.hist(df["bedrooms"])
plt.title("histogram of bedrooms")

plt.subplot(4, 3, 9)
plt.hist(df["buildingArea"])
plt.title("histogram of buildingArea")

plt.subplot(4, 3, 10)
plt.hist(df["garageSpaces"])
plt.title("histogram of garageSpaces")



plt.show()

plt.figure(figsize=(20,20))

# Histograms of the winsorized data
plt.subplot(4, 3, 1)
plt.hist(df["win_price"])
plt.title("histogram of win_price")

plt.subplot(4, 3, 2)
plt.hist(df["win_pricePerSquareFoot"])
plt.title("histogram of win_pricePerSquareFoot")

plt.subplot(4, 3, 3)
plt.hist(df["win_time"])
plt.title("histogram of win_time")

plt.subplot(4, 3, 4)
plt.hist(df["win_yearBuilt"])
plt.title("histogram of win_yearBuilt")

plt.subplot(4, 3, 5)
plt.hist(df["win_livingArea"])
plt.title("histogram of win_livingArea")

plt.subplot(4, 3, 6)
plt.hist(df["win_livingAreaValue"])
plt.title("histogram of win_livingAreaValue")

plt.subplot(4, 3, 7)
plt.hist(df["win_bathrooms"])
plt.title("histogram of win_bathrooms")

plt.subplot(4, 3, 8)
plt.hist(df["win_bedrooms"])
plt.title("histogram of win_bedrooms")

plt.subplot(4, 3, 9)
plt.hist(df["win_buildingArea"])
plt.title("histogram of win_buildingArea")

plt.subplot(4, 3, 10)
plt.hist(df["win_garageSpaces"])
plt.title("histogram of win_garageSpaces")

plt.show()

#Now to visualize the categorical variables

#Categorical variables for bar chart:
#is_bankOwned, is_forAuction, event, hasBadGeocode, lotAreaUnits, hasGarage, pool, spa,
#isNewConstruction, hasPetsAllowed, homeType

#Categorical variables for word cloud:
#city, county, levels

plt.figure(figsize=(15,5))
plt.barh(df.groupby("is_bankOwned")["is_bankOwned"].count().index,
       df.groupby("is_bankOwned")["is_bankOwned"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of is_bankOwned")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("is_forAuction")["is_forAuction"].count().index,
       df.groupby("is_forAuction")["is_forAuction"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of is_forAuction")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("event")["event"].count().index,
       df.groupby("event")["event"].count(),
       color=["red","blue", "orange", "grey", "pink", "green"])
plt.title("horizontal bar chart of event")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("hasBadGeocode")["hasBadGeocode"].count().index,
       df.groupby("hasBadGeocode")["hasBadGeocode"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of hasBadGeocode")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("lotAreaUnits")["lotAreaUnits"].count().index,
       df.groupby("lotAreaUnits")["lotAreaUnits"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of lotAreaUnits")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("hasGarage")["hasGarage"].count().index,
       df.groupby("hasGarage")["hasGarage"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of hasGarage")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("pool")["pool"].count().index,
       df.groupby("pool")["pool"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of pool")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("spa")["spa"].count().index,
       df.groupby("spa")["spa"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of spa")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("isNewConstruction")["isNewConstruction"].count().index,
       df.groupby("isNewConstruction")["isNewConstruction"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of isNewConstruction")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("hasPetsAllowed")["hasPetsAllowed"].count().index,
       df.groupby("hasPetsAllowed")["hasPetsAllowed"].count(),
       color=["red","blue"])
plt.title("horizontal bar chart of hasPetsAllowed")

plt.show()

plt.figure(figsize=(15,5))
plt.barh(df.groupby("homeType")["homeType"].count().index,
       df.groupby("homeType")["homeType"].count(),
       color=["red","blue", "orange", "grey", "pink", "green"])
plt.title("horizontal bar chart of homeType")

plt.show()

pip install wordcloud

from wordcloud import WordCloud

wordcloud = WordCloud(background_color="orange").generate(" ".join(df["city"]))
plt.figure(figsize=(15,10))
# Display the generated image
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

plt.show()

wordcloud = WordCloud(background_color="orange").generate(" ".join(df["county"]))
plt.figure(figsize=(15,10))
# Display the generated image
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

plt.show()

wordcloud = WordCloud(background_color="orange").generate(" ".join(df["levels"]))
plt.figure(figsize=(15,10))
# Display the generated image
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")

plt.show()

#Now let's do some bivariate analysis to seem if there are any relatonships between variables

#We will start with continuous-continuous variables

#The continuous variables we are exploring are:
#"price","pricePerSquareFoot","countyId","cityId","time","yearBuilt",
        #"zipcode","livingArea","livingAreaValue","bathrooms",
        #"bedrooms","buildingArea","garageSpaces"

#We want to examine the relationship to price first, as that's our target variable

plt.figure(figsize=(20,25))

plt.subplot(4,3,1)
plt.scatter(df["price"], df["pricePerSquareFoot"])
plt.title("price vs pricePerSquareFoot")

plt.subplot(4,3,2)
plt.scatter(df["price"], df["countyId"])
plt.title("price vs countyId")

plt.subplot(4,3,3)
plt.scatter(df["price"], df["cityId"])
plt.title("price vs cityId")

plt.subplot(4,3,4)
plt.scatter(df["price"], df["time"])
plt.title("price vs time")

plt.subplot(4,3,5)
plt.scatter(df["price"], df["yearBuilt"])
plt.title("price vs yearBuilt")

plt.subplot(4,3,6)
plt.scatter(df["price"], df["zipcode"])
plt.title("price vs zipcode")

plt.subplot(4,3,7)
plt.scatter(df["price"], df["livingArea"])
plt.title("price vs livingArea")

plt.subplot(4,3,8)
plt.scatter(df["price"], df["livingAreaValue"])
plt.title("price vs livingAreaValue")

plt.subplot(4,3,9)
plt.scatter(df["price"], df["bathrooms"])
plt.title("price vs bathrooms")

plt.subplot(4,3,10)
plt.scatter(df["price"], df["bedrooms"])
plt.title("price vs bedrooms")

plt.subplot(4,3,11)
plt.scatter(df["price"], df["buildingArea"])
plt.title("price vs buildingArea")

plt.subplot(4,3,12)
plt.scatter(df["price"], df["garageSpaces"])
plt.title("price vs garageSpaces")

plt.show()

#Ok, there seems to be a positive relationship between price and bathrooms

#Let's examine the correlation coefficient to determine a if there is any 
#statistical significance

df[continuous].corr()

#To further examine the correlation, let's make a heatmap

# Make the correlation matrices
corrmat = df.corr()

# Heat maps are a great way to get
# a quick visual read on a big correlation matrix.

# Draw the heat map using seaborn
plt.figure(figsize=(14,14))
sns.heatmap(corrmat, square=True, annot=True, linewidths=.5)
plt.title("correlation matrix")

plt.show()

#Hmm that didn't seem to help much. Probably just go off of the
#statistical outcome from above

#Now let's move on to continuous-categorical pairs

#Categorical variables:
#is_bankOwned, is_forAuction, event, hasBadGeocode, lotAreaUnits, hasGarage, pool, spa,
#isNewConstruction, hasPetsAllowed, homeType

df.groupby("is_bankOwned").mean()

#0 is no, 1 is yes

df.groupby("is_forAuction").mean()

df.groupby("event").mean()

df.groupby("hasBadGeocode").mean()

df.groupby("lotAreaUnits").mean()

df.groupby("hasGarage").mean()

df.groupby("pool").mean()

df.groupby("spa").mean()

df.groupby("isNewConstruction").mean()

df.groupby("hasPetsAllowed").mean()

df.groupby("homeType").mean()

"""Now let's do some bivariate analysis of categorical-categorical pairs"""

df[categorical].head()

pd.crosstab(df["hasGarage"], df["is_forAuction"])

pd.crosstab(df["hasGarage"], df["hasBadGeocode"])

pd.crosstab(df["hasGarage"], df["lotAreaUnits"])

pd.crosstab(df["hasGarage"], df["is_bankOwned"])

pd.crosstab(df["hasGarage"], df["pool"])

pd.crosstab(df["hasGarage"], df["spa"])

pd.crosstab(df["hasGarage"], df["isNewConstruction"])

pd.crosstab(df["hasGarage"], df["hasPetsAllowed"])

pd.crosstab(df["hasGarage"], df["homeType"])

"""Now let's plot our categorical pairs to visualize"""

sns.countplot(y="is_bankOwned", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="is_forAuction", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="hasBadGeocode", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="lotAreaUnits", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="pool", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="spa", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="isNewConstruction", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="hasPetsAllowed", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

sns.countplot(y="homeType", hue="hasGarage", data=df, palette="Greens_d")
plt.show()

"""Overall, it looks like most of our data are properties that have a garage"""

"""Feature Engineering

Let's apply a Box-Cox transformation to make variables more normally distributed"""

"""from scipy.stats import boxcox

time_boxcox,_ = boxcox(df["time"])
price_boxcox,_ = boxcox(df["price"])
pricePerSquareFoot_boxcox,_ = boxcox(df["pricePerSquareFoot"])
yearBuilt_boxcox,_ = boxcox(df["yearBuilt"])
livingArea_boxcox,_ = boxcox(df["livingArea"])
livingAreaValue_boxcox,_ = boxcox(df["livingAreaValue"])
bathrooms_boxcox,_ = boxcox(df["bathrooms"])
bedrooms_boxcox,_ = boxcox(df["bedrooms"])
garageSpaces_boxcox,_ = boxcox(df["garageSpaces"])


plt.figure(figsize=(18,5))

# Histograms of the Box-Cox-transformed data
plt.subplot(3, 3, 1)
plt.hist(time_boxcox)
plt.title("histogram of time (box-cox transformed)")

plt.subplot(3, 3, 2)
plt.hist(price_boxcox)
plt.title("histogram of video price (box-cox transformed)")

plt.subplot(3, 3, 3)
plt.hist(pricePerSquareFoot_boxcox)
plt.title("histogram of price per square foot (box-cox transformed)")

plt.subplot(3, 3, 4)
plt.hist(yearBuilt_boxcox)
plt.title("histogram of year built (box-cox transformed)")

plt.subplot(3, 3, 5)
plt.hist(livingArea_boxcox)
plt.title("histogram of living area (box-cox transformed)")

plt.subplot(3, 3, 6)
plt.hist(livingAreaValue_boxcox)
plt.title("histogram of living area value (box-cox transformed)")

plt.subplot(3, 3, 7)
plt.hist(bathrooms_boxcox)
plt.title("histogram of bathrooms (box-cox transformed)")

plt.subplot(3, 3, 8)
plt.hist(bedrooms_boxcox)
plt.title("histogram of bedrooms (box-cox transformed)")

plt.subplot(3, 3, 9)
plt.hist(garageSpaces_boxcox)
plt.title("histogram of garage spaces (box-cox transformed)")

plt.show()"""

"""The box-cox transformation did not work so we will move on without it

Next, we will use normality tests and jarque bera tests to test for normality of distribution
of our raw and winsorized variables. Some machine learning models prefer more normally distributed variables."""


from scipy.stats import jarque_bera
from scipy.stats import normaltest

jb_stats = jarque_bera(df["time"])
norm_stats = normaltest(df["time"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["price"])
norm_stats = normaltest(df["price"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["pricePerSquareFoot"])
norm_stats = normaltest(df["pricePerSquareFoot"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["yearBuilt"])
norm_stats = normaltest(df["yearBuilt"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["livingArea"])
norm_stats = normaltest(df["livingArea"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["livingAreaValue"])
norm_stats = normaltest(df["livingAreaValue"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["bathrooms"])
norm_stats = normaltest(df["bathrooms"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["bedrooms"])
norm_stats = normaltest(df["bedrooms"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["garageSpaces"])
norm_stats = normaltest(df["garageSpaces"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

"""Due to the p value for both tests being under 0.05 we can see that our raw data is not 
normally distributed. Let's perform the same test on our winsorized data."""

#df["win_pricePerSquareFoot"] = winsorize(df["pricePerSquareFoot"], (0, 0.10))
#"""df["win_id"] = winsorize(df["id"], (0, 0.10))
#df["win_countyId"] = winsorize(df["countyId"], (0, 0.10))
#df["win_cityId"] = winsorize(df["cityId"], (0, 0.10))"""
#df["win_time"] = winsorize(df["time"], (0, 0.10))
#df["win_yearBuilt"] = winsorize(df["yearBuilt"], (0, 0.10))
#"""df["win_zipcode"] = winsorize(df["zipcode"], (0, 0.10))
#df["win_longitude"] = winsorize(df["longitude"], (0, 0.10))
#df["win_latitude"] = winsorize(df["latitude"], (0, 0.10))"""
#df["win_livingArea"] = winsorize(df["livingArea"], (0, 0.10))
#df["win_livingAreaValue"] = winsorize(df["livingAreaValue"], (0, 0.10))
#df["win_bathrooms"] = winsorize(df["bathrooms"], (0, 0.10))
#df["win_bedrooms"] = winsorize(df["bedrooms"], (0, 0.10))
#df["win_buildingArea"] = winsorize(df["buildingArea"], (0, 0.10))
#df["win_garageSpaces"] = winsorize(df["garageSpaces"], (0, 0.10))

jb_stats = jarque_bera(df["win_time"])
norm_stats = normaltest(df["win_time"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_price"])
norm_stats = normaltest(df["win_price"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_pricePerSquareFoot"])
norm_stats = normaltest(df["win_pricePerSquareFoot"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_yearBuilt"])
norm_stats = normaltest(df["win_yearBuilt"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_livingArea"])
norm_stats = normaltest(df["win_livingArea"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_livingAreaValue"])
norm_stats = normaltest(df["win_livingAreaValue"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_bathrooms"])
norm_stats = normaltest(df["win_bathrooms"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_bedrooms"])
norm_stats = normaltest(df["win_bedrooms"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

jb_stats = jarque_bera(df["win_garageSpaces"])
norm_stats = normaltest(df["win_garageSpaces"])

print("Jarque-Bera test statistics is {0} and p value is {1}".format(jb_stats[0], jb_stats[1]))
print("Normality test statistics is {0} and p value is {1}".format(norm_stats[0], norm_stats[1]))

"""While bedrooms and bathrooms seem to show a small amount of normality, we still have nothing
that will qualify as a normal distribution of data.

Now, let's move on to do some PCA"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sqlalchemy import create_engine
import warnings

warnings.filterwarnings('ignore')
sns.set_style("whitegrid")

# Normalize the data so that all variables have a mean of 0 and standard deviation
# of 1.
X = StandardScaler().fit_transform(df.select_dtypes(include='float64'))

# The NumPy covariance function assumes that variables are represented by rows,
# not columns. So transpose `X`.
Xt = X.T
Cx = np.cov(Xt)
print('Covariance Matrix:\n', Cx)

# Calculating eigenvalues and eigenvectors
eig_val_cov, eig_vec_cov = np.linalg.eig(Cx)

# Inspecting the eigenvalues and eigenvectors.
for i in range(len(eig_val_cov)):
    eigvec_cov = eig_vec_cov[:, i].T
    print('Eigenvector {}: \n{}'.format(i + 1, eigvec_cov))
    print('Eigenvalue {}: {}'.format(i + 1, eig_val_cov[i]))
    print(40 * '-')

print(
    'The proportion of total variance in the dataset explained by each',
    'component calculated by hand\n',
    eig_val_cov / sum(eig_val_cov)
)

"""How many components to keep?

Strategy: Keep components with eigenvalues greater than 1, because they add value 
(more than a single variable would).

If we use that strategy, we will use eigenvalues: 1, 2, 3, 4, 5, 6, 20

Or...

We can use a scree plot to visually inspect the plot and keep all the components 
whose eigenvalue falls above the point where the slope of the line changes the most 
drastically. This point is also called the elbow."""

# Print scree plot
plt.plot(eig_val_cov)
plt.show()

print(eig_val_cov)

"""The scree plot says to only keep the first 6 eigenvalues.

Now let's confirm with sklearn function."""

sklearn_pca = PCA(n_components=7)
Y_sklearn = sklearn_pca.fit_transform(X)

P = eig_vec_cov[:, 0]

# Transform `X` into `Y`.
Y = P.T.dot(Xt)

print(
    'The percentage of total variance in the dataset explained by each',
    'component from Sklearn PCA.\n',
    sklearn_pca.explained_variance_ratio_
)

"""Now let's try some modeling. First up, Linear Regression"""

from sklearn import linear_model
import warnings
warnings.filterwarnings('ignore')

"""First we need to transform all of the categorical 
variables into numerical variables with 'get dummies'"""

"""df['is_bankOwned'] = df['is_bankOwned'].map({1: 'yes', 0: 'no'})
df['is_forAuction'] = df['is_forAuction'].map({1: 'yes', 0: 'no'})
df['hasBadGeocode'] = df['hasBadGeocode'].map({1: 'yes', 0: 'no'})
df['hasGarage'] = df['hasGarage'].map({1: 'yes', 0: 'no'})
df['pool'] = df['pool'].map({1: 'yes', 0: 'no'})
df['spa'] = df['spa'].map({1: 'yes', 0: 'no'})
df['isNewConstruction'] = df['isNewConstruction'].map({1: 'yes', 0: 'no'})
df['hasPetsAllowed'] = df['hasPetsAllowed'].map({1: 'yes', 0: 'no'})"""

df["is_bankOwned_num"] = pd.get_dummies(df.is_bankOwned, drop_first=True)
df["is_forAuction_num"] = pd.get_dummies(df.is_forAuction, drop_first=True)
#df["hasBadGeocode_num"] = pd.get_dummies(df.hasBadGeocode, drop_first=True)
df["hasGarage_num"] = pd.get_dummies(df.hasGarage, drop_first=True)
df["pool_num"] = pd.get_dummies(df.pool, drop_first=True)
df["spa_num"] = pd.get_dummies(df.spa, drop_first=True)
df["isNewConstruction_num"] = pd.get_dummies(df.isNewConstruction, drop_first=True)
df["hasPetsAllowed_num"] = pd.get_dummies(df.hasPetsAllowed, drop_first=True)

"""Let's see how that worked"""

df.head()

"""Now let's handle some multiclass categorical data. Let's split up homeType into multiple
numerical variables."""

homeType_df = pd.get_dummies(df['homeType'])
homeType_df.head()

"""Now let's merge the new numerical version of homeType with the original data"""

df2 = df.drop(['homeType'], axis =1).merge(homeType_df,left_index=True, right_index=True)
df2.head()

"""To begin modeling, we will start with number of bathrooms, number of bedrooms and price of 
the property using an sklearn model"""

# `Y` is the target variable
Y = df2['price']
# `X` is the feature set which includes the
# `is_male` and `is_smoker` variables
X = df2[['bathrooms','bedrooms']]

# Create a `LinearRegression` model object
# from scikit-learn's linear_model module.
lrm = linear_model.LinearRegression()

# Fit method estimates the coefficients using OLS
lrm.fit(X, Y)

# Inspect the results
print('\nCoefficients: \n', lrm.coef_)
print('\nIntercept: \n', lrm.intercept_)

"""Since we get a more detailed results of our regression using the statsmodel api,
we will now run another OLS regression that way"""

import statsmodels.api as sm

# `Y` is the target variable
Y = df2['price']

# `X` is the feature set
X = df2[['bathrooms','bedrooms', 'yearBuilt', 'pool_num']]

# Add a constant to the model because it's best practice
# to do so every time!
X = sm.add_constant(X)

# Fit an OLS model using statsmodels
results = sm.OLS(Y, X).fit()

# Print the summary results
print(results.summary())

"""Now let's evaluate how our model performed.

F-test
By observing the F-test, we can see that our F-statistic is 2302 and the associated p-value is
very close to 0. This means that our feature set does add some information to the reduced model
(or null hypothesis) and the model is useful in explaining 'price'.

R-squared
The R-squared is 0.25, which signals that the model does not make very good predicitons and does
not explain much of the variance in the outcome variable ('price').

Overall, this model did not perform very well at all.

Since there are no hyperparameters to tune, we will move on to test another type of model.
"""

# Commented out IPython magic to ensure Python compatibility.
"""Next up, we will use a K-nearest neighbors model to evaluate our data.

First, to classify price range that is within the average families budget, we will 'cut'
the price variable into pieces and label them as inBudget, underBudget, or overBudget. then we will
meausure the labels against bedrooms and bathrooms to observe trends. The average family's budget
will be between 400k and 800k in USD."""

df2['priceLabel'] = pd.cut(df2['price'], bins=[0, 400000, 800000, float('Inf')], labels=['underBudget', 'inBudget', 'overBudget'])

import scipy
# %matplotlib inline

# Look at the data
plt.figure(figsize=(14,14))
plt.scatter(
    df2[df2['priceLabel'] == 'underBudget'].bedrooms,
    df2[df2['priceLabel'] == 'underBudget'].bathrooms,
    color='red'
)
plt.scatter(
    df2[df2['priceLabel'] == 'inBudget'].bedrooms,
    df2[df2['priceLabel'] == 'inBudget'].bathrooms,
    color='blue'
)
plt.scatter(
    df2[df2['priceLabel'] == 'overBudget'].bedrooms,
    df2[df2['priceLabel'] == 'overBudget'].bathrooms,
    color='green'
)
plt.legend(['underBudger', 'inBudget', 'overBudget'])
plt.title('inBudget, underBudget, and overBudget Characteristics')
plt.xlabel('Bedrooms')
plt.ylabel('Bathrooms')
plt.show()

from sklearn.neighbors import KNeighborsClassifier
neighbors = KNeighborsClassifier(n_neighbors=1)
X = df2[['bathrooms', 'bedrooms']]
Y = df2.priceLabel
neighbors.fit(X,Y)

## Predict for a song with 3 Bathrooms and 4 Bedrooms.
neighbors.predict([[3, 4]])

neighbors.predict([[0, 6]])

neighbors.predict([[2, 3]])

"""Now let's tune the model a bit by adjusting the k to 5"""

neighbors = KNeighborsClassifier(n_neighbors=5)
X = df2[['bathrooms', 'bedrooms']]
Y = df2.priceLabel
neighbors.fit(X,Y)

## Predict for a song with 3 Bathrooms and 4 Bedrooms.
neighbors.predict([[3, 4]])

neighbors.predict([[0, 6]])

#2 Bathrooms and 3 Bedrooms
neighbors.predict([[2, 3]])

"""Based on the KNN model above with k=5 and the sample provided, we can accurately predict that a house with 2 Bathrooms and
3 Bedrooms is considered in-budget for a family with a budget of 400k to 800k to spend on a house.

Comparing the results of the tested models


Results of Linear Regression Model:

R-squared
The R-squared value from the OLS regression we performed is 0.25, which signals that the 
model does not make very good predicitons and does not explain much of the variance in the 
outcome variable ('price').

Linear Regression models do not have hyperparameters to tune to improve their performance, so
there was no increase in performance.

Overall, this model did not perform very well at all.


Results of KNN Model:

With k=1, the model was not very helpful in predicting whether a house was under, in, or over budget.
However, when we tuned the k hyperparameter to 5, the model became much more helpful. We found that,
using the model we could reasonably predict that a house with 2 bathrooms and 3 bedrooms 
is considered in-budget for a family with a budget of 400k to 800k to spend on a house. This insight
will help the average family narrow their search for a home and tailor their expectations as to
what their options are. Using this model, the average family will be able to do a more in-depth
inquiry into what their options are and what is reasonable to ask for from the housing market that
is described in this dataset, which is from 2021 so is close to current and acurate for the present.

It is for the reasons listed above that we select the KNN model as the best-performing model.
"""